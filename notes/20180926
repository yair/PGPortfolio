todo --
play with params - the ones you changed last time, window size, 200k iterations, whatever.
Also, test again with orig config
I mean, how come test after training is 0.4 and after rolling 1.8?
But yeah, stuck on 99% BTC, this is not functioning properly.

84 - wd=1e-5 on last, 1e-6 on dense, 31 coins, 80k iters, 8% test, 2015/07/01-2018/09/24 -- bias falls to -.35, then rises to +.35 and could go on, test and train are essentially the same with benefit around .45, though reaches 1.7 on BT. Is rolling training better?
Oops, deleted it.
84 - same, wd reduced to 1e-8 on both dense and last, and increasted iters to 200k. bias falls to -.16, then rises to -.08 and stabilizes, test and train the same, benefit at .6, and .6 on BT.  Back test total assets is 2.5.
85 - same, wd increased to 1e-7 on both, 200k iter, 41 window size. bias falls to -.12 and stabilizes and -.03. Test and train the same, benefit at .4 and, but 1.8 on BT.BT retest on 85 gives the same answer, which means we might want to save a checkpoint after it finishes. Back test total assets is 2.32.
86 - same, 61 window size. Better, surprisingly. Bias drops to -.2 then rises to .4 and continues. Test and train the same, benefit at 1.5 on train and bt, but noisy. Backtest total assets is 3.44.
87 - same, learning rate decreased to 0.0002. Bias drops to -.1, then rises to .8 and continues. Test and train the same, benefit at 1.5 on train and bt, less noisy. Back test total assets is 3.46.
88 - same, 41 coins. Bias drops to -.3, rises to .7 and continues. Test and train the same, benefit at 1.6 on train, 1.8 on test. Back test total assets is 3.62.
Take that? Did. Not bad. Prob is, it traded only once in a week. Prolly because of the high btc bias. In general, I think we want it positive and low. That way it is mostly in btc, but trades often. Question is what we can do to affect it. Maybe play with weight decay.
89 - same, wd=1e-5 on last, 1e-6 on dense, range extended to 29.9. Bias drops to -.35, then rises to -.1 and continues. Test and train the same, benefit on 1.6. Backtest didn't log. :/
90 - same, wd=1e6 on last, 1e-7 on dense. Bias drops to -.25, rises to -.1 and continues. Test and train the same, again 1.6 (odd!) on train, unknown on BT. BT total assets 2.83BTC.
91 - same, btc_bias initialized to 1 instead of 0 (orig, matches initial portfolio). Crashed - OOM allocating a [52251,3,41,60] tensor.
92 - same, reducing batch size from 109 to 59. Didn't help, same OOP.
93 - same, returning batch size to 109, switching fast train on. Worked. Bias drops to .95 and climbs to 1.25. Test and train the same at 1.45, unk for bt. BT total assets 1.7BTC.
94 - 88 copy, range extended to 1.10.18 and no fast train (btc_bias initted to 0 again). Lousy results - bias falls to -.09 and stays. Test and train at .95. BT total assets 1.3BTC. Ah, no weight decay was wrong.
95 - 88 copy, range back till 24.9, weight decay fixed, no fast train (i.e. 88 exact copy). Surprisingly, that worked. 3.87 total bt assets, though btc_bias dropped to -.3 and got up back to 0.
96 - 88 copy, but 1000000 steps, and new delistings. OOM again, but after 800k steps still didn't breach benefit=1. btc_bias flattened at -.27.
97 - 88 copy, new delisting, 200k steps. This is stupid. Now it gets to benefit=1.6. And BT just exited without doing anything. btc_bias droppd to -.3 and rose to .7 to continue. benefit for train and test finished at ~1.4.
Running it manually, BT exited without an error after one iteration, or 52092 experiences, that is probably the size of that tensor.
98 - 97 copy, date range clipped from bottom -- 2015/08/01 - 2018/09/24. First run reached 1.6 but crashed. Second run only reached .9.
99 - 98 copy, training learning rate increased back to 0.0028. btc_bias drops to -.6, rises to 0 and continues. Benefit on train/test reaches 1.6, on bt ~1.7. BTTA 3.26BTC.
100 - 99 copy, new consumptions. btc_bias drops to -.3, rises to 1.3 and continues. benefit 1.6 on both, 1.9 on bt. BTTA 4.38. :o.
101 - 100 copy. 125000 decay_steps. btc_bias drops to -.14, rises to .15 and continues. benefit 1.6 on both, BT incomplete (benfit ~1.75, BTTA 3.3 after 4072/4474 iterations).
102 - 101 copy. Traning learning rate increased to 0.0005, 50000 decay_steps. Not much of a change. btc_bias drops to -.27, rises to .27. Benefit on both 1.6, 1.8 on bt. BTTA 3.19btc.
103 - 102 copy, training learning rate 0.002. Worse - took 150000 steps to cross benefit one. btc_bias dropped to -3.5, then -1.5, then -4.5 and continued. Benefit on both 1.6, higher on bt, and BTTA=2.62btc.
104 - 103 copy, 25000 decay_steps. Lol, that killed it completely. 0 trades, everything's 1...
===
105 - 100 copy, date extended to 4.10.2018, test portion reduced to 5%. GPU OOM
105 - 105 copy, from date moved to 2015/08/01. Slow to start working (113k steps to benefit 1). btc_bias drops to -.4, then rises to 0. Benefit 1.35 on both, 1.4 on bt, BTTA 1.72BTC. Nothing to compare it to, but it seems ok.
106 - 106 copy, weight decays 5e-9 on dense and 5e-8 on eiie. Better. btc_bias rises to .2, drops to -2.8 and continues. Benefit 1.42 on training, 1.45 on BT. BTTA 1.54BTC. So is it better or worse?!
107 - 106 copy, weight decause 5e-8 on dense and 5e-7 on eiie. 68k steps to benefit=1. btc_bias drops to -.2, then rises to .6 and continues. benefit 1.4 on training and BT. BTTA 2.2BTC. Nice! Nice? :/
108 - 107 copy, weight decay 5e-7 on dense and 5e-6 on eiie. failed to reach benefit=1, though did the dip just before 200k so maybe?
dense decay --              5e-9        5e-8        1e-7        5e-7
eiie            5e-8        1.54 (106)  F (113)     -           ??
decay           1e-7        -           2.29 (117)  1.72 (105)  -
                5e-7        2.19 (109)  2.20 (107)  -           2.14 (112)
                5e-6        ??          1.56 (111)  -           F (108)
109 - 108 copy. dense decay 5e-9 eiie decay 5e-7. btc_bias drops to -.4, rises to .05 and continues. benefit 1.4 on training (inflection @99k), 1.35 on bt. BTTA 2.19BTC
110 - 108 copy. dense decay 5e-8 eiie decay 5e-8. tensorboard only reaches 28k. OOM :|
111 - 108 copy. dense decay 5e-8 eiie decay 5e-6. btc_bias drops to -1, rises to .85. benefit 1.2 on training but might go on (inflection @182k), 1.25 on bt. BTTA 1.56BTC
112 - 108 copy. dense decay 5e-7 eiie decay 5e-7. btc_bias drops to -.25, rises to .2 and continues. benefit 1.44 on training (inflection @76k), 1.4 on bt. BTTA 2.14BTC
113 - 110 rerun. Failed mid-inflection.
next - rerun 111 with more steps. 113 as well?
114 - 111 copy, 400k steps. inflection 126k :/ OOM 346k. :(
115 - 114 rerun. inflection 94k, but converged to benefit 1.2 instead of 1.4. :/
116 - 108 copy. dense decay 5e-8 eiie decay 1e-7. OOM in !fast_train.
117 - 116 copy, fast_train. btc_bias drops to -.2, the rises to 1.8. Benefit 1.38 on training, inflection 101k, BTTA 2.29.
next - play with weight decay of conv layer.
118 - 117 copy, conv weight decay 1e-9. Inflection 78k. Benefit 1.37 on training. BTTA 2.11.
119 - 117 copy, conv weight decay 1e-8. Inflection 120k. Benefit 1.37 on training. BTTA 1.79.
120 - 117 copy, conv weight decay 1e-7. Quenched.
121 - 117 copy, conv weight decay 1e-6. Quenched. Forgetaboutit.
next - rerun without btcusd.
122 - 117 copy, conv weight decay 0, btcusdt banned. Inflection 64k. Benefit 1.27. BTTA 2.50. :D Can we do better? Benefit is still low.
123 - 122 copy, eiie decay 5e-7. Inflection 153k. Benefit 1.22. BTTA 1.89.
124 - 122 copy, dense decay 5e-9 eiie decay 5e-7. Inflection 89k. Benefit 1.38. BTTA 2.01.
125 - 105 copy, sans usdt. Benefit reached 4e-5. That's a first. 8|
126 - 122 copy, loss function 4. Benefit shot straight to 0. Aborted.
127 - 122 copy, loss function 5. Reached 0 even faster.
128 - 122 copy, loss function 6 (rerun). Inflection 67k. Benefit 1.29. BTTA 2.17.
129 - 122 copy, loss function 7. Inflection 141k. Benefit 1.32. Loss 5e-4. BTTA 1.95.
130 - 122 copy, loss function 8. Crashed, need to fix -- AttributeError: 'NNAgent' object has no attribute '_NNAgent__commission_ratio'
131 - 126 copy, cause stopped. No inflection, benefit 0. Loss -5.6e-3. BTTA 51.2. WAT DA FUCK!
132 - 127 copy. Loss drops to -5e-3, then rises to -3.8e-3. BTTA 35.2. God.
133 - 130 copy, explicit 0.0025 comission. Loss -8e-4. BTTA 1731. 8|
134 - 122 copy, loss function 9. Inflection 34k. Loss -8e-4. BTTA 1470.
next - 134 + usdt.
135 - 134 copy, with btcusdt. Inflection 39k. Loss -8e-4. BTTA 1094.
136 - 135 copy, doubled consumption vector. Inflection 39k.  Loss -8e-4. BTTA 1041. This is bonkers, will require extra research.
next - 80k steps.
next - Can we please not use all the experiences all the time in the GPU so we can use the full date range again? Don't even have to be intelligently subsampled, just rotating random portions of it. :/
next - what is decay_rate? Can Xavier init help? TF provides lotsa decay options. We'll need to research that...
next - 5% bt period (0% for prod?), recent range, play with hyperparams to optimize. Maybe different initializations, it takes too long to start converging.
       What does decay_steps do? Oh! Definitely play with that! 125000 might be a good place to start. And why does it decay exponentially? Shouldn't be 1/x or 1/x^2?
next - new loss function - (loss6 - .001*omega[0]^2), to supress (1,0,0,...).
