Should be first cloud trained release, to explore features that our beyond our current memory constraints.
todo:
1. Fix namespace issue with tcn dropout (i.e. fugly hack)       <=== dropped tcn for now, and the rest of the list.
2. Move DB from sqlite to PG.
3. Consider externalizing some of the hyperparams for automatic searches.
4. Explore attention, highway (expressway? whatever) and gated features of tcn.
5. explore modifying loss function to make a better policy (A3C, trusted region, other PG improvement. The bootcamp videos are a good start).
6. Explore moving to dropout to another regularization technique which is presumably more 'rl friendly'.

Main thing for this release is going to be experimenting with scaling network inputs by inverse consumption to try and encourage trading more liquid coins.
210 - 122 clone. Ah, crap, we still reduce batch sizes. Redone. Didn't converge. Meh. BTTA 1.95BTC/2943
211 - same, window size 31. Breakout @29k, but a measly -3e-5 loss afterwards. Very timid, BTTA=1 for the first 2000 periods (3 months!) but finished BTTA=1.32BTC/2943. Good enough for experimenting with inverse consumption scaling?

DB data is OHLCV, not normalized. Where is normalization done?
get_global_panel returns same OHLCV.
I think the normalization is in line 230 of pack_samples -- y = M[:, :, :, -1] / M[:, 0, None, :, -2] ... Nope, that's just y - it's just a per-coin profit/loss calc.
Dunno.
Maybe in _build_network. Yup that seems to be it. We need to transform the consumption vector into a tf constant and push it there.
Then, we take the normalized prices, substruct one, divide by a constant times the consumption vector, then add one again. Yeah.
But how to do the division? This is a 4D tensor divided by a 1D tensor, we need to make sure it's divided along the correct dimension.
We can tile it to 4D along the missing dimensions. :/ Maybe the correct multiplication of ones with the same shape?
Weird results. We might need to mod our loss function as well.

212 - 211 copy, with price differences scaled by inverse consumption vector - inflection 72k, dropped down to almost -5e-5, but very noisy and finished at about 0. BTTA 1.25BTC/2943, which is worse.
But I think the general idea is correct -- the thing seems to mainly trade BTCUSD! We never saw it trade that! Maybe we should divide by the square root of the consumptions? And FCT (makes sense! But it's also pretty liquid - at the middle of the list) ZEC, EOS, XMR, the dollar again...
So, square root of the consumptions, and put the variable batch sizes back.
213 - 212 copy, square root scaling. Loss stuck at +4e-5 until 100k before jumping up almost to +1e-4, then inflects at 137k. It reaches -6e-5, but becomes pretty noisy, finishing somewhere around -8.5~-6.5e-5. In backtest, it shows a tendency to lose a lot of the trades (like #212).  BTTA=1.67BTC after trading etc, usd and others.
214 - same, fixed normalization. Less noisy. Inflected only at 168k but dropped to -6e-5. In BT, traded XRP, USD, BCH, GAME (8th from the end!), ETC, BTS and more. BTTA=.75BTC/2943. Not bad! But still skewed to the beginning of the coin list.
215 - same, sqrt(sqrt()), just for fun. Inflection on 85k, stabilizing at ~-6e-5. Portfolio on test set ~1.2BTC (is that proof that BT training is better?) Traded PASC (#27), (#16), #19, #3, #27, #34, #6, #36, #17, etc. Pretty balanced, I guess. BTTA=1.80BTC, but pretty deep drawdowns.
216 - same, without the +1 after consumption multiplication. Inflection @59k, much less noisy, stabilized @-6.5e-5 (1.25BTC on test set). BT coins not traded are on e-16 instead of e-6. Pretty, but BTTA is only 1.47BTC. Maybe we need to play with the hyperparameters.
217 - same, 100k steps. Inflection in 48k, finished @-7e-5. BT background noise ~e-10. BTTA=1.47BTC again.
218 -? combined with #195 -- 5 batching epochs, 400 batch size, 2e-5 buffer bias, 0.5 decay rate, 120k decay steps, 0.002 learning rate, 320k steps. Inflection at 3k 8|, bottomed quickly @-6e-5, then slowly climbed to above 0 at the end of the first batch epoch, then rising to stabilize @~+3e-5, yet BTTA=2.30BTC/2973. Weird.
219 -? same, sqrt (sqrt ()) => sqrt (). Inflection @3k, but right back up, creeping up to +4e-5, yet BTTA=2.76BTC.
220 - same, sqrt () => linear consumptions. Inflection @5k, stabilized on ~-2.5e-5, which looks better. Deep drawdowns, finished with BTTA=1.20BTC.
221 - 219 copy, network += 1. Null result. Forgetaboutit.
222 - 219 copy, 100k steps. Inflection @4k, quickly stabilizing @~-7.5e-5, best yet, but only BTTA=1.47BTC. So is it better? Is it worse?
223 - Same. A bit noisier. BTTA=1.70BTC, which just goes to show.
224 - Same, 200k steps. BTTA 1.37. A lemon?
225 - #219 copy, no consumption division. Inflection @5k. BTTA=1.44.
226 - 320k steps, no consumption division, no network-=1. BTTA=1.39.
227 -> same, just the -=1. Stabilized close to -1e-4, best yet. BTTA=2.06BTC, not too shabby either.
228 - same, no btcusd. -5e-5, BTTA=1.40BTC. :/
229 -? same, sqrt (consumptions) (and no usdt). Stabilized on -9e-5, then catastrphically crashed at 37k to stabilize at +5e-5. BT does a lot of trades and finishes with BTTA=2.62BTC, which is a lot. I dunno what to do with these high loss, high benefit sessions.
230 -? same, 300k steps. Catastrophy @100k to +4e-5, BTTA=2.41BTC.
231 - same, 240k steps. Inflection @4k, stable @-9e-5, no internet - crashed.
232 -? rerun. Inflected and rose, finishing ~+5e-5. BTTA=2.88BTC/2973.
233 - same, with btcusdt. Quickly down to -8e-5, then up to stabilize @+2e-5. More drawdowns, more confused. BTTA=2.19BTC.
234 - same, without btcusdt, 4 batching levels (400, 200, 100, 50). Dropped, rose and stabilized around 0. loss -2e-5->+1e-5. BTTA=2.69BTC.
235 - 231 copy. Dense weight decay 3e-7 (instead of 5e-8). Loss down to -8e-5 and stabilized negative at -6e-5, but BTTA=1.57BTC. Are these two things always going to go in opposite directions?
236 - Same, 5 batch epochs. Infl. @3k, stable @-7e-5. Cata @143k, ended ~+1e-5. BTTA=1.93BTC.
237 -? Same, Dense weight decay 1e-7. Infl. @3k, min -7e-5. Cata @10k, stable @+3e-5. BTTA=2.81BTC/2973.
238 - Same, Dense weight decay 1e-7 (oops, no change). Infl. @3k, stable @-8e-5, no catastrophy, finished -7e-5. BTTA=1.53BTC. I guess they are inversely correlated. 8|
239 -? Same, Dense weight decay 1e-7 (oops, no change). Infl. @3k, stable @-8e-5, Cata @31k, stable +5e-5. BTTA=2.44BTC.
240 - Same Dense weight decay 1e-8. Infl. @2k. to -9e-5, stays at -8e-5. No cata. BTTA=1.64BTC.
Conv layer has no weight decays in our config file. What did we change? Probably the dense layer.
241 - Dense weight decays back to 5e-8, EIIE weight decay 1e-6. Infl. @4k to -8e-5, climbed to stabilize at -7e-5. BTTA=1.54BTC. Like you'd expect for a non-catastrophic run by now. 50 minutes to run.
Night run - Back to old weight decays. Bracket batch sizes (8 levels?)
242 - Back to default weight decays, batch size 30 (instead of 400), single epoch. Infl. @11k to -7e-5, avging -5e-5. BTTA=1.53BTC.
243 - Same, batch size 50. Infl. @7k to -8e-5, avging -7e-5. BTTA=1.52BTC.
244 -? Same, batch size 100. Infl. @8k to -8e-5, cata @50k avging +4e-5. BTTA=2.86BTC.
245 - Same, batch size 300. Infl. @4k to -8e-5, stable to -6e-5. BTTA=1.58BTC.
246 - Same, batch size 500. Infl. @3k to -8e-5, cata @13k. Stable at 0, then +3e-5, then +4, then +5e-5, ending on +6e-5. BTTA=2.24BTC.
247 - Same, batch size 1000. Infl. @3k to -7e-5, cata @21k. Straight up to +1e-4, ending at +1.3e-4 (!). BTTA=2.75BTC.
248 - Same, batch size 3000. Infl. @3k to -9e-5, stable crawl to -5e-5. BTTA=1.65BTC.
886 minutes. :|
249 - Batch size 1000, 5k steps. Infl. @3k to -6.7e-5. BTTA=1.66BTC.
250 - Batch size 1000, 7k steps. Infl. @3k to -8e-5. BTTA=1.75BTC.
251 - Batch size 1000, 10k steps. Infl. @2k to -8e-5. BTTA=1.50BTC.
252 - Batch size 1000, 15k steps. Infl. @3k to -6e-5. BTTA=1.51BTC
253 - Batch size 1000, 20k steps. Infl. @3k to -8e-5 stab. -6e-5. BTTA=1.52BTC
254 - Batch size 1000, 30k steps. Infl. @3k to -8e-5 stab. -6e-5. BTTA=1.36BTC
255 - Batch size 1000, 40k steps. Infl. @2k to -8e-5, then -9.5e-5. BTTA=1.75BTC
256 - Batch size 1000, 50k steps. Infl. @3k to -9e-5, then -1.5e-5, then -7e-5, cata @27k stab. +2e-5. BTTA=2.54BTC
257 - Batch size 1000, 70k steps. Infl. @3k to -7.5e-5, stab -4e-5. I had enough.
258 - Batch size 1000, 100k steps.
259 - Batch size 1000, 150k steps.
So we get this behaviour of sometimes overfitting (BTTA=~1.5BTC) and sometimes catastrophing (BTTA=~2.8BTC) with constant batch sizes as well.

Got new consumptions with 16 levels. Let's run our best with those.     // What were the batch epoch settings for these? single?
260 - 244 copy. Infl. @6k to a stable -1e-4. BTTA 1.81BTC. How do we get it to cata?
261 - Same. Batch size 200. Infl. @2k to -9e-5, stab -6e-5. BTTA=1.58BTC
262 -> Same. Batch size 400. Infl. @2k to -9e-5, stab -7e-5, cata @120k, stab -1e-5. BTTA=3.43BTC
263 - Same. Batch size 600. Infl. @2k to -6e-5, cata @25k, stab +5e-5. BTTA=crashed. Rerun --
263 rerun - Infl. @5k to -9e-5. cata @20k stab +9e-5. BTTA=crashed. Rerun --
263 rererun - Infl. @3k to to -5e-5 cata @60k stab +2e-5. BTTA=3.57BTC.
264 - Same. Batch size 800. BTTA=1.34BTC
265 - Same. Batch size 1000. Infl. @3k to -4e-5, cata @20k to +9e-5 stab +4e-5. BTTA=3.29BTC

266 - 4 batch epochs, batch size 400 - Infl. @2k to -9e-5, cata @80k, crashed.
267 - same, batch size 600 - Infl. @3k to -9e-5, cata @150k stab. +2e-5. BTTA=3.05BTC
268 - same, batch size 800 - Infl. @3k to -8e-5, cata @35k stab +7e-5. BTTA=2.27BTC
269 - same, batch size 1000 - Infl. @2k to -5e-5, stab -2e-5. BTTA=1.80BTC
270 - same, batch size 1500 = Infl. @4k to -8e-5, stab -5e-5. BTTA=1.60BTC

Changed date to till 7.1.2019.

271 - same as 263 - batch size 1500, 4 batch epochs (wasn't 263 single epoch?!) - Infl. @2k to -8e-5, infl. @31k stab. +6e-5. BTTA=4.47BTC
Next - same, with single batching epoch
272 - 271 copy, single epoch, batch size 1500. Infl. @3k to -1.4e-3 (!) but never catastrophed. Stab @-1.1@e-3 (also (!)). BTTA=2.32BTC. 4.5 hours.
273 - 272 copy (single batching epoch), batch size 1000 - Infl. @1k (!) to -1.3e-4, cata @32k, stab @+8e-4. BTTA=4.36BTC
274 - 272 copy (single batching epoch), batch size 800 - Infl. @2k to -7e-5, cata@31k, stab @+1e-4. BTTA=4.28BTC
275 - 272 copy (single batching epoch), batch size 600 - Infl. @3k to -1.1e-4, cata @47k, stab @+3e-5. BTTA=3.92BTC
276 -> 272 copy (single batching epoch), batch size 400 - Infl. @1k to -1.6e-4 (!), cata @28k, stab 6e-5. BTTA=5.16BTC
277 - 272 copy (single batching epoch), batch size 200 - Infl. @2k to -1.3e-4, no cata, stab@-8e-5. BTTA=1.93BTC
278 ->> 272 copy (single batching epoch), batch size 100 - Infl. @3k to -1.2e-4, false cata @99k, true one @125k, spanning +6e-5~+1.5e-4. BTTA=7.03BTC :O

279 - 278 copy (single epoch, 100 batch size), decay_steps=40k - Infl. @3k to -1.3e-4, no cata, stab @-1e-4. BTTA=2.13BTC
280 - ditto, decay_steps=60k - Infl. @3k to -1.2e-4, cata @42k, ranging +3e-5~+1.1e-4. BTTA=4.58BTC
281 - ditto, decay_steps=80k - Infl. @3k to -1.5e-4, no cata - stab @-1.2e-4. BTTA=2.19BTC
282 - ditto, decay_steps=100k - Infl. @3k to -1.1e-4, cata @42k, ranging at -4e-5~+4e-5. BTTA=4.70BTC
283 -> ditto, decay_steps=140k - Infl. @3k to -1.3e-4, cata @59k, up to 0~+1e-4. BTTA=5.90BTC
284 - ditto, decay_steps=180k - Infl. @3k to -1.5e-4, no cata, stab. +1e-4. BTTA=1.58BTC

Put BCH on banlist (not sure if it auto-bans sv and abc, need to get them on the training set to find out.)

285 -> ditto, decay_steps=110k - Infl. @4k to -1.5e-4, cata @25k, wide band around 0. BTTA=5.16BTC -- deployed.
286 - ditto, decay_steps=120k - Infl. @4k to -1.3e-4, cata @42k, ranging -5e-5~+5e-5. BTTA=4.27BTC
287 - ditto, decay_steps=130k - Infl. @7k to -1.3e-4, no cata, stab. -1.2e-4. BTTA=2.12BTC
288 - ditto, decay_steps=140k - Infl. @5k to -1.3e-4, no cata, stab. -1.2e-4. BTTA=1.67BTC

289 - single batch epoch of 200, decay steps=60k - Infl. @2k, never cata stab. -1.3e-4. BTTA=2.10BTC
290 - single batch epoch of 200, decay steps=80k - Infl. @2k, never cata stab. -1.4e-4. BTTA=1.99BTC
291 - single batch epoch of 200, decay steps=100k - Infl. @2k, never cata stab -1.1e-4. BTTA=1.82BTC
292 - single batch epoch of 200, decay steps=120k - Infl. @1k, never cata stab -6e-5. BTTA=1.98BTC
293 - single batch epoch of 200, decay steps=140k - Infl. @1k cata 74k to +5e-5~+1e-4. BTTA=4.98BTC
294 - single batch epoch of 200, decay steps=160k - Infl. @1k, never cata stab -6e-5. BTTA=1.84BTC
295 - single batch epoch of 200, decay steps=180k - Infl. @1k, never cata stab -8e-5. BTTA=1.77BTC

296 - single batch epoch of 200, decays_steps=120k, learning_rate 2e-4 - Infl. @48k to -7e-5. Never catastrophed, but slowly rose to ~-2e-5. BTTA=2.99BTC
297 - single batch epoch of 200, decays_steps=120k, learning_rate 5e-4 - Infl. @22k to -8e-5, second infl. @43k to -1.3e-4, no cata stab. -9e-5. BTTA=2.41BTC
298 - single batch epoch of 200, decays_steps=120k, learning_rate 1e-3 - Infl. @3k to -1.4e-4, no cata stab. -.9e-5. BTTA=2.17BTC
299 - single batch epoch of 200, decays_steps=120k, learning_rate 2e-3 - Infl. @2k to -1.3e-4, cata @195k (!) to +9e-5. BTTA=2.78BTC
300 - single batch epoch of 200, decays_steps=120k, learning_rate 5e-3 - Infl. @1k to -1.3e-4, cata @9k (:|) to +2e-4 ranging 0~+2.5e-4. BTTA=3.73BTC
301 -> single batch epoch of 200, decays_steps=120k, learning_rate 1e-2 - Infl. @1k to -1.5e-4, false cata @45k, cata @70k to +1.7e-4, infl no. 2 @112k to -1e-4, cata @142k stab. ~+8e-5. BTTA=5.58BTC . A bit more than an hour per run.
 
302 - single batch epoch of 200, decay_steps=80k, learning_rate=5e-3 - Infl. @1k to -1.3e-4, cata @103k stab. +1e-4. BTTA=3.07BTC
303 - single batch epoch of 200, decay_steps=80k, learning_rate=1e-2 - Infl. @1k to -1.4e-4, no cata stab. -1.1e-4. BTTA=1.95BTC
304 - single batch epoch of 200, decay_steps=80k, learning_rate=2e-2 - Infl. @1k to -1.9e-4 (!), no cata stab. -7e-5. BTTA=1.88BTC
305 -> single batch epoch of 200, decay_steps=160k, learning_rate=5e-3 - Infl. @1k to -1.3e-4, cata @40k to +2.1e-4 (!) stab. +1.2e-4. BTTA=5.51BTC
306 - single batch epoch of 200, decay_steps=160k, learning_rate=1e-2 - Infl. @1k to -1.5e-4, cata @154k to +1.1e-4 stab. ~0. BTTA=4.05BTC
307 - single batch epoch of 200, decay_steps=160k, learning_rate=2e-2 - Infl. @2k to -1.9e-4 (!) false catas @24k &@140k stab -1e-4. BTTA=2.30BTC

308 -> single batch epoch of 200, decay_rate=1 (no decay), learning rate = 5e-4 - Infl. @18k to -7e-5 cata @100k stab +5e-5. BTTA=5.46BTC
309 - single batch epoch of 200, decay_rate=1 (no decay), learning rate = 1e-3 - Infl. @3k to -1.3e-4 cata @60k to +1.3e-4 re-infl. @145k to -1.1e-4 stab -9e-5. BTTA=3.44BTC.
310 - single batch epoch of 200, decay_rate=1 (no decay), learning rate = 2e-3 - Infl. @2k to -1.3e-4 never cata. stab. -1e-4. BTTA=2.27BTC
311 - single batch epoch of 200, decay_rate=1 (no decay), learning rate = 5e-3 - Infl. @1k to -1.5e-4 never cata. stab. -8e-5. BTTA=2.15BTC
312 - single batch epoch of 200, decay_rate=1 (no decay), learning rate = 1e-2 - Infl. @1k to -1.5e-4 false cata @78k stab -5e-5~+5e-5. BTTA=1.76BTC
313 - single batch epoch of 200, decay_rate=1 (no decay), learning rate = 2e-2 - Infl. @3k to -1.3e-4 cata @85k to 1.7e-3 (lol) then inflects again, ends -1.8e-4~+2e-4. BTTA=1.71BTC
308 is no better than top others, but the low learning rate might mean it's stabler. 309 is the highest BTTA to finish on a negative loss, I think.

Switched to 1546944911 consumptions and extended date to 2019/1/11.
314 - single batch epoch of 200, decay_rate=1, learning rate = 2e-4 - infl. @18k to -1.4e-4, climbed slowly to -1.1e-4 (over-training?). BTTA=2.14BTC
315 - same, 60k training steps - infl. @18k to -1.4e-4. BTTA=2.10BTC
316 - same, 80k training steps, batch size of 100 - infl. @25k to -1.3e-4. BTTA=2.08BTC
317 - same, batch size of 400 - infl. @26k to -9e-5, up to -5e-5 (would've cata'ed?). BTTA=3.46BTC
318 - same, batch size of 800 - infl. @16k to -6e-5, up to -4e-5. BTTA=3.04BTC
319 - same, batch size of 1200 - infl. @21k to -8e-5, up to 0. BTTA=3.65BTC
320 - same, batch size of 1600 - infl. @30k to -1.4e-4. BTTA=2.24BTC
321 -> same, batch size of 2400 - infl. @13k to -8e-5, up to 0. BTTA=5.07BTC
322 - same, batch size of 3200 - infl. @26k to -1.3e-4 stab. -1.3e-4. BTTA=2.09BTC
323 ->> same, batch size of 4800 - infl. @16k to -9e-5, up to -2e-5. BTTA=Dunno. Just training took 8 hours. Actually, let it run, and it's blooming amazing! BTTA=7.25BTC!

Ok... Can we get that kind of performance without it taking 16 hours? Maybe by fiddling with learning rate. Let's experiment with 1200 batch size.
324 - same, batch size of 1200, learning rate 1e-4 - infl @58k to -7e-5, then up to -5e-5. Would've cata'ed if given the option, methinks. BTTA=3.54BTC.
325 -> same, batch size of 1200, learning rate 5e-4 - infl @8k to -9e-5 cata @28k to +1e-4. BTTA=5.97BTC
326 ->> same, batch size of 1200, learning rate 1e-3 - infl @3k to -1.3e-4 cata @18k to +1.2e-4. BTTA=7.17BTC
327 ->> same, batch size of 1200, learning rate 2e-3 - infl @2k to -1.4e-4 cata 16k to +8e-5, down to +1e-5. BTTA=7.07BTC
328 - same, batch size of 1200, learning rate 5e-3 - infl @1k to -1.4e-4 never cata stab -8e-5. BTTA=2.27BTC
329 -> same, batch size of 1600, learning rate 5e-4 - infl @8k to -7e-5 cata @27k stab -2e-5~+2e-5. BTTA=6.53BTC
330 -?? same, batch size of 1600, learning rate 1e-3 - infl @4k to -1.3e-4 cata @17k to +1.2e-4 stab +7e-5. BTTA=2.82BTC after 1150/3065 steps, i.e. very promising (was 2.67BTC for #327, 2.81BTC for #323).
331 -> #330 rerun - infl @5k (!) to -4e-5 cata @12k to +1e-4, re-infl @28k to -1.2e-4, re-cata @70k to +4e-5. BTTA=1.87BTC after 1050/3065, so this is much worse than the orig. BTTA=5.26
332 - same, batch size of 1600, learning rate 2e-3 - infl @2k to -1.3e-4 never cata stab. -7e-5. BTTA=2.08BTC
333 - same, batch size of 1600, learning rate 5e-3 - infl @2k to -1e-4 cata @8k to +2e-4. BTTA=1.33 after 536/3065, was 1.46BTC for 330, so nah. buhbye.

334 - same, no consumption weighting, bs=1200 lr=2e-4 - infl @41k, down to -7e-5 and keeps going. Might've had an interesting life given enough time. Very high floor in BT (>~1.3e-3 at the beginning, >~1.4e-6 by the end) BTTA=1.81BTC
335 - same, no consumption weighting, bs=1200 lr=5e-4 - infl @10k no cata stab. -1e-4. Again could prolly get somewhere. BTTA=1.88BTC
336 - same, no consumption weighting, bs=1200 lr=1e-3 - infl @6k to -1e-4. Seemed about to cata but ended on -7e-5. Won't bother to BT.
337 - same, no consumption weighting, bs=1200 lr=2e-3 - infl @4k to -1.1e-4 no cata stab. -5e-5. BTTA=1.59BTC
338 - same, no consumption weighting, bs=1200 lr=5e-3 - infl @2k to -6e-5 cata @51k to +6e-5. BTTA=1.92BTC
339 - same, no consumption weighting, bs=1200 lr=1e-2 - infl @1k to -6e-5 cata @42k to +8e-5 stab +4e-5. BTTA=1.56BTC
So this is utter rubbish. Would un-sqrt'ed consumption scaling fare better? Good q for another time. Let's deploy our best so far, which is 326 (it's almost the best, but 4800 batch online training without GPU scares me a bit).

We still want to test different consumption weightings of inputs. It currently seems too biased towards the heavy-weights. Which might be good, or not.

Why do we use the same batch sizes in back testing?
Next - take 1000 sample batches, and bracket early stopping. Why 1000? 100 gives better results.
Why does BT takes longer the larger the batches are? Aren't they just for initial training?
Very possible that low loss leads to low benefit because of overfitting. Yup, and weight decays is the L2 params. We really want to augment our data (5 min shifts).
Also, larger batches might help. (the 4 epoch test was semi successful.)
Play with learning rate and decay.
Until 237, max unweird BTTA was 227 @2.06BTC, yet it was very close to weirding. Ugh.
What if we just trained with big batches the whole way?
Important! Save network after BT and use it for live.
Try increasing L2. -- didn't help.
Succeessful trades are all at the end of BT. We want to verify that it's market behaviour and not that the bot only learns during BT.
Think about logging consumptions. How will that look? How to normalize it?
