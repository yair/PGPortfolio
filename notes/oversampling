This is an old idea for augmenting our data.
Currently we are using window_size (31) samples taken every global_period (1800 seconds) from a date divisible by global_period.
The idea is to take these samples at other offsets divisible by our storage_period (300 seconds).
So instead of a window at 13:00, 13:30, 14:00 etc., we'll also have windows that look like 13:05, 13:35, 14:05. This is augmentation from real data we have and might lead to better performance.

The backtesting part, however will need to remain as is, the overlap between them should be exactly 0, live should not be affected, etc.
This basically means that instead of DataMatrices calling get_training_set and get_test_set and basically slicing parts of the global set. Two (three?) different sets will need to be generated in a way that will conserve the current behaviour.
Also note that while the tester only uses this latter set, its learnertrainer uses both sets to continually retrain the network.
So we'll need to keep to the same indexing scheme, at least, or something like that. I.e. recreate the same unified global panel, but with two different calls to the database, one that samples every storage_period, and the other by global_period.
Note that this will also require changes to __pack_samples, as it will need to skip samples every 6.

The place I'd like to start, though, is with explicitly specifying training and testing periods. Eh, really? Wouldn't that be a nuisance?

Actually, we can keep the set unified, sample every storage_period, and only add __pack_samples_augment . Still need to be careful with indices. Maybe, as first step, just use the new flag ('train_set_augment') to switch between working fully on storage_period and global_period.

Crap. Group by means we skip ahead of the previous group. Can we look ahead without skipping?
Ok, don't use group_by, there's a better one - 'over'.

SELECT MarketDate,
       ClosingPrice,
       AVG(ClosingPrice) OVER (ORDER BY MarketDate ASC ROWS 9 PRECEDING) AS MA10
FROM   @DailyQuote

                    sql = ("SELECT date_norm, MAX(high)" +                          # the old expression
                           " FROM (SELECT date+{period}-(date%{period})"    <- retraces backward +30m->...->+55m
                           " AS date_norm, high, coin FROM History)"
                           " WHERE date_norm>={start} and date_norm<={end} and coin=\"{coin}\""
                           " GROUP BY date_norm".format(
                                period=period,start=start,end=end,coin=coin))

                    sql = ("SELECT date+{period} AS date_norm," +                   # the proposed new expression for high
                           "       MAX(high) OVER (ORDER BY date ASC ROWS {further_samples} FOLLOWING) AS high," +
                           "       coin " +
                           "FROM   History " +
                           "WHERE  date_norm>={start} and date_norm<={end} and coin=\"{coin}\"".format(
                               period=period,start=start,end=end,
                               coin=coin,further_samples=(period//self.__storage_period - 1)))
Let's first try it on the current scheme -
                    sql = ("SELECT date+{period} AS date_norm," +                   # the proposed new expression for high
                           "       MAX(high) OVER (ORDER BY date ASC ROWS {further_samples} FOLLOWING) AS high," +
                           "       coin " +
                           "FROM   History " +
                           "WHERE  date_norm>={start} and date_norm<={end} and coin=\"{coin}\" and date_norm%{period}=0".format(
                               period=period,start=start,end=end,
                               coin=coin,further_samples=(period//self.__storage_period - 1)))
                    sql = ("SELECT date+{period} AS date_norm," +                   # the proposed new expression for low
                           "       MIN(low) OVER (ORDER BY date ASC ROWS {further_samples} FOLLOWING) AS low," +
                           "       coin " +
                           "FROM   History " +
                           "WHERE  date_norm>={start} and date_norm<={end} and coin=\"{coin}\" and date_norm%{period}=0".format(
                               period=period,start=start,end=end,
                               coin=coin,further_samples=(period//self.__storage_period - 1)))

Fook, can't use FOLLOWING (not sure why), but can use PRECEDING. So... how do we do that?
date_norm is the date at the _end_ of the candle (date+period).
It groups by date_norm, which is defined as (date + period - (date % period))
date    -> date+period
date+5  -> date+period
...
date+25 -> date+period
date+30 -> date+2*period
so we need to group (date, date+period-storage_period) under the name date+period, using preceding. Cool.
We start with date+25, count 5 back and call it date+period. Right?
SELECT date+{period}-{storage_period} as date_norm, etc. etc.

Ok... this gives different results. Why?
We are looking at eth 30m high values on 26.1.2019
get_data -- 17:00 - 0.032527
tradingview -- 16:30 - 0.03252748 (but that is ok because tv indexes by candle start and we by its end, right?)
get_data2 -- 17:00 - 0.032480
Odd. This is probably the candle at 16:35, but it's surrounded by higher candles, so how can it be the max?
Similary the 15:35 candle looks like the max of what get_data2 calls 16:30 (0.032448), but it's also surrounded by higher candles. Is our OVER clause not working?
get_data's high for 16:30 is 0.032465 which is the tv candle at 16:00, and indeed the higest in (16:00-16:29).
So first of all advance by ~25 minutes?
Now with offset 2 * period - storage_period.
Crap. It just ignores fields that I filter out with WHERE, which makes perfect sense. Do I need GROUP BY after all? Seems like.
Ok, back to the drawing board.
Also, take into account you'll be looping to generate the... or am I? No, I don't want to loop. I want not to filter on time for starters.
Crapping out on not in index. What does that mean?
Changed end to end-period. nope.
Changed start to start+period. nope.
This is odd. Isn't it about the boundaries?
High worked and low didn't?!
Not the sql failed. The squeeze failed. WAT.
Squeeze removes 1-length dims from an array. It's a numpy method. Ah, ok, so it's still high. We haven't reached low.
Before squeeze, shape=(376116, 1)
Error is --
KeyError: "DatetimeIndex(['2015-06-30 17:35:00', '2015-06-30 17:40:00',\n               '2015-06-30 17:45:00', '2015-06-30 17:55:00',\n               '2015-06-30 18:05:00', '2015-06-30 18:10:00',\n               '2015-06-30 18:15:00', '2015-06-30 18:20:00',\n               '2015-06-30 18:25:00', '2015-06-30 18:35:00',\n               ...\n
        '2019-01-26 15:35:00', '2019-01-26 15:40:00',\n               '2019-01-26 15:45:00', '2019-01-26 15:50:00',\n               '2019-01-26 15:55:00', '2019-01-26 16:05:00',\n               '2019-01-26 16:10:00', '2019-01-26 16:15:00',\n               '2019-01-26 16:20:00', '2019-01-26 16:25:00'],\n              dtype='datetime64[ns]', name='date_norm', length=313429, freq=None) not in index"
Solutions might be here -- https://stackoverflow.com/questions/38462920/pandas-keyerror-value-not-in-index
This is possibly due to a blank space sneaking into a header. Ah! It says so in the error message - the field name is date_norm, and the values it shows don't have the round-period entries. So it's because the 'close' line only read round period entries. We need to change it as well.
That is good. It'll make sure that we have the same boundaries across our stuff, I think.
Ok, that bombed. Where was the index created? Line 115. We'll need to change that each time we modify augmentation too.
That works. :O
When running again - mod the index and switch to get_data.
eth high 2015-08-08 (hour, data price, data2 date range for that price)
05:30 - 50.00000 - 06:05 - 06:30 - +7-+12
06:00 - 0.012800 - 06:35 - 06:50 - +7-+10
06:30 - 0.007190 - 07:20 - 07:30 - +10-+12
07:00 - 0.006590 - 07:50 - 08:00 - +10-+12
07:30 - 0.006900 - 08:15 - 08:30 - +9-+12
So it has to be +10, right?
So instead of the current date_offset=2*period-self.__storage_period we want data_offset=self.__storage_period, right?
Yeah, that seems correct. Indeed, XMR correct too. That's enough.
Ok, now to the filtering in pack_samples. Crap. This is gonna involve some hard thinkin'.
Okay. First line - indexs. We only need to figure out the date offset of our indexs and we're golden, right? Now how do we do that? Get one and check?
Yeah. We don't need get_submatrix 'cause that'll get us a whole window... Crap. get_submatrix also assumes consecutivity, who else?
But anyway, not now. What do we get from self.__global_data.values? Does it have dates? Prolly not. It's just...
Let's just print a line. Yeah, nope, just prices. So where can we get this? get_data just drops it and that's it. Or is it in the index? It is, and it starts with 'start', so this is where we're at.
Let's just try to use that and... But start is on a round date and we somehow start at 5:30? Is that a non-GMT thing again? 'Cause I'll be pissed.
---> 18.2.19
We need to come up with the correct index list on all pack_samples invocations. They're all in datamatrices, so at least there's that.
Okay. All current calls to pack_samples contain simple ranges. We need to either 
- do nothing, if we are running without augmentation,
- prune all indices not divisible by 6 (or generate them that way to begin with), if we do use augmentation but don't want it right now (testing, live),
- prune and augment - oversample with all (6) possible offsets.
How are the indices arranged in the third case? How do we make sure not to create subsets across time reversal boundaries if we arrange the offsets one after the other.
Another option is postpone the pruning and the offsetting to when the samples are used. That's not a good idea...
Ok, so pruning.
Why is skip indices called so much? It's generated online while training, not once before!
Also, why is expand indices called with aug=True, skip=True while training? 
Where is the first call from? get_test_set, called from trader_trainer, possibly for online measurement of training progress. Makes sense that it's called with skip, then.
Sadly, it's not aligned to 30 minutes, so we need to realign it first.
Eh, just because the index has a non-zero 6-mod doesn't mean the date is not divisible without remainder by 30. Feh.
Need to print self.__global_data.values[:, :, ind]
Did. It has the data, but not dates. Dates are not stored, they're simply calculated as the index offset from $start. And that one _is_ correctly offset, so it seems to mean that our odd indices indeed refer to odd times. Bummer. Also, why?! How?!
Crap. The third param in the list slice format is a modulu. How do you offset it? Or is it a step? I get conflicting docs...
Ok, solved as i[5-i[5]%6]. But training results are very different even though they're all on skip. Why?
Back testing is also much poorer, basically didn't match any patterns. Did we get an offset difference between the different markets?
Expand indices is called every training step, to get a new random point in history. They have a modern slant for some reason. Can't see anything before 2018, even though we're supposed to be looking since 2015. Are we doing the historical log decay thing? I thought that was only for bt. Because if we do, and the index list is 6 times as long, we're looking at a sixth of the history.
Buffer bias, that's the thing. Should be 6 times lower. During both training and testing.
Which, by the way, we should have played with long ago.
Nope. Wasn't it. Much healthier spread, training same rubbish.
What do we need to do to debug this? We can compare timestamps to the charts if see if they make sense.
Let's print the data as well...
I think there was another place we needed to modify. Don't remember where... get_submatrix maybe?
get_test_set (and prolly training too) needs to be shortened.
Ok, done and it doesn't crash. TRaining charts look better though still far from the unaugmented version (to which, since we're forcing skip, we should be identical). But it definitely looks alive. B)
That's one funky shape. :O But BT is not that bad. Interesting.
We are far from done testing, but let's try without forcing skipping.
Ah - still need to remove the broken date areas
Ok. This needed to be done previously as well, to a more limited extent - removing a window length portion from the end of the training set.
Where was that done?
First of all, let's look at how the test set is done -
1. Client calls get_test_set
2. get_test_set calls test_indices and sends that to __pack_samples
3. test_indices does both the skip and the backing off.
4. pack_samples calls __expand_indices skips again but doesn't do the backing off.
So this is rubbish.
Originally,
1. Client calls get_test_set
2. get_test_set calls test_indices and sends that to __pack_samples
3. test_indices does the backing off
4. __pack_samples did nothing.
So, it makes sense that the *_indices functions do all the hard lifting, and __pack_samples, get_submatrix and all the other wonders are only modified to generically be able to handle the two modes.
So remove extraneous code from __expand_indices? Better yet, remove this function completely. Or call it from *_indices. :/
Becuase get_training_set had no indices function and did the backing off itself. [:(.
For the live set, we'll just disable augmentation when running live. This'll shorten our reaction time. Done.
Just a sec, divide data also did the backoff on train indices. What a mess.
When and how is the buffer bias used? Because right now it might be working completely against us.
438 - Test run without augmentation (but after fixing the double backoff in training - so much better than the original benchmark (435) though nothing changed. Unclear. BTTA=1.96
439 - Same same, with aug. Very high loss while training, but looks profitable in BT. Had to cut short. Rerun.
440 - Rerun. Training identical BTTA=1.85BTC
441 - Same same, bs=400 - BTTA=1.98BTC. Loss reached +1.8e-3, completely out of whack. Are we using this thing properly?
442 - bs=200 no aug (need to change the buffer bias!) - BTTA=1.46BTC
443 - bs=200 aug - 1.67BTC
Changed consumptions from 1550472569
444 - bs=400 no aug - BTTA=1.51BTC
445 - bs=800 no aug - BTTA=1.55BTC
446 - bs=800 aug - BTTA=1.99BTC
447 -> bs=1200 aug - BTTA=2.66BTC :O
Changed dates till 22.2.19
448 - bs=1200 no aug - BTTA=1.57BTC
449 - bs=1600 aug - BTTA=1.98BTC after 4 hours. phew.
Changed to generic aug factor code (to allow for 2h runs), date to Feb. 25th
450 - 443 rerun (new date and code) - BTTA=0.79BTC. Something is wrong. :|
451 - same sans aug (i.e. 442 dupe with new code, date and consumptions, which by the way we need to look at) - nope, it's not the code change. :| Crap, forgot to change the buffer bias. :( but BTTA=1.71BTC :/
452 - 442 exact replica - same consumptions (1548239800) and date range (till 2019/02/18), and the correct bloomin' bias. - BTTA=1.55BTC
453 - same same, with aug, i.e. exact 443 replica. - BTTA=1.61BTC
close enough.
454 - 442 copy, new consumptions (bs=200, no aug) - BTTA=1.39BTC
455 - 443 copy, new consumptions (bs=200, with aug) - BTTA=1.62BTC.
No biggie here either. New consumptions might be a bit worse.
456 - 450 rerun. New consumptions, new code, new date, bs=200, +aug. BTTA=0.97BTC
So it's the date. Bleh.
457 - new consumpt., new code, date to 18.2, bs=1200, period=2h. Bigloop takes 5 minutes. That's not so good. (note that consumptions are wrong, true BTTA will be higher). Still, BTTA=1.17BTC. Let's forget about that for a while.
458 - bs=1200, period=30m, +aug, lr=1e-3. BTTA=1.53BTC
459 - bs=1200, period=30m, +aug, lr=1e-4. BTTA=1.88BTC
460 -> lr=2e-3 - BTTA=2.22BTC
461 -> lr=5e-4 - BTTA=2.03BTC
462 - lr=5e-5 - BTTA=1.34BTC
463 - lr=2e-5 - (default weight decays - 5e-8 on dense, 1e-7 on eiie) - BTTA=1.23BTC 
464 -> lr=2.8e-4, bs=1200, dense wd = 1e-8, eiie wd = 2e-8 - BTTA=2.25BTC
465 -> dense wd=5e-9, eiie wd=1e-8 - BTTA=2.26BTC
466 - dense wd=2e-9 eiie wd=5e-9 - BTTA=2.58BTC
467 - dense wd=1e-7 eiie wd=2e-7 - BTTA=2.10BTC
468 - dense wd=2e-7 eiie wd=5e-7 - BTTA=2.38BTC
469 - dense wd=1e-8 eiie wd=1e-7 - BTTA=2.04BTC
470 - dense wd=1e-7 eiie wd=1e-8 - BTTA=2.10BTC rerun 2.40BTC
471 - dense wd=1e-9 eiie wd=2e-9 - BTTA=1.98BTC rerun 2.08BTC
472 0 dense wd=0 eiie wd=0 - BTTA=2.07 rerun 2.28BTC
Ok, too low decay is not good. Let's explore the best range. What is the best range?
We had >2 results for (5e-8, 1e-7), (1e-8, 2e-8), (5e-9, 1e-8), (2e-9, 5e-9), (1e-7, 2e-7), (2e-7, 5e-7), (1e-8, 1e-7), (1e-7, 1e-8), (0, 0)
473 - dense wd=2e-8 eiie wd=5e-8 - BTTA=1.85BTC
474 - dense wd=5e-9 eiie wd=5e-8 - BTTA=1.97BTC
475 - dense wd=2e-8 eiie wd=1e-8 - BTTA=2.14BTC
476 - dense wd=1e-8 eiie wd=1e-7 - BTTA=2.13BTC
477 - dense wd=2e-9 eiie wd=2e-8 - BTTA=1.96BTC
478 - dense wd=1e-8 eiie wd=5e-9 - BTTA=2.00BTC
479 - dense wd=5e-8 eiie wd=5e-9 - BTTA=2.03BTC
480 - dense wd=5e-8 eiie wd=2e-8 - BTTA=2.28BTC
481 - dense wd=5e-9 eiie wd=2e-9 - BTTA=2.29BTC
482 - dense wd=1e-7 eiie wd=5e-8 - BTTA=2.28BTC
483 - dnese wd=1e-9 eiie wd=1e-8 - BTTA=2.29BTC
484 - dense wd=2e-8 eiie wd=2e-7 - BTTA=2.43BTC
485 - dense wd=5e-8 eiie wd=5e-7 - BTTA=1.78BTC
486 - dense wd=2e-7 eiie wd=1e-7 - BTTA=2.24BTC
487 - dense wd=5e-7 eiie wd=2e-7 - BTTA=1.96BTC
488 - dense wd=5e-7 eiie wd=1e-8 - BTTA=1.97BTC
489 - dense wd=1e-9 eiie wd=2e-7 - BTTA=2.35BTC
dense eiie  1e-9        2e-9        5e-9        1e-8        2e-8        5e-8        1e-7        2e-7        5e-7
1e-9        2.18 (~472) 1.98,2.08 (471)         2.29 (483)                                      2.35 (489)
2e-9                                2.58 (466)              1.96 (477)              2.04 (469)
5e-9                    2.29 (481)              2.26 (465)              1.97 (474)
1e-8                                2.00 (478)              2.25 (464)              2.13 (476)
2e-8                                            2.14 (475)              1.85 (473)              2.43 (484)
5e-8                                2.03 (479)              2.28 (480)              2.66,2.20 (447,490)     1.78 (485)
1e-7                                            2.10,2.40 (470)         2.28 (482)              2.10 (467)
2e-7                                                                                2.24 (486)              2.38 (468)
5e-7                                            1.97 (488)                                      1.96 (487)
Should probably run a few more 447 clones for statistics. What should I twiddle for chaos's sake? Let's try nothing.
490 - default wds (5e-8, 1e-7), window size=31 (447 clone) - BTTA=2.20BTC
The conclusion here is that there's too much noise and too little variation between the weight decay values, and the default is prolly fine.
491 - ws=27 - BTTA=1.93BTC
492 -> ws=23 - BTTA=3.03BTC
493 - ws=35 - BTTA=2.28BTC
494 - ws=19 - BTTA=2.14BTC
495 - ws=21 - BTTA=2.12BTC
496 - ws=25 - BTTA=2.02BTC
497 - ws=22 - BTTA=2.52BTC
498 - ws=24 - BTTA=1.72BTC
Is this huge difference between window sizes of 23 and 24 somehow an effect of the CNN? Let's rerun 492.
499 - 492 rerun (ws=23) - BTTA=2.55BTC. Still loks good.
500 - ws=23 steps = 60k - BTTA=1.95BTC
501 - steps = 120k - BTTA=2.40BTC
502 - steps = 160k - BTTA=2.50BTC
503 -> steps = 200k - BTTA=2.91BTC
Changed to date to 5.3.2019
504 - 492 rerun - BTTA=1.51BTC
Next - rerun best results from previous date range. (503, 497, 466, 447)
(505 - binance test run)
506 - 503 rerun. BTTA=1.52BTC
507 - 497 rerun (ws=22, steps=80k) - BTTA=1.24BTC
508 - 466 rerun (ws=31, dense wd=2e-9 eiie wd=5e-9) - BTTA=1.26BTC
509 - 447 rerun (ws=31, 5e-8, 1e-7, 1200, 80k) - BTTA=1.44BTC
510 - 502 rerun (ws=23 steps=160k) - BTTA=1.35BTC
511 - 501 rerun (ws=23 steps=120k) - BTTA=1.49BTC

TODO:
- Save model after backtesting
- Set a different batch size for backtesting
- What exactly are the remining consequences of the non-monotonous index list? How badly do we need to mask the ending of each round (and do we really need masking, or would snipping the end be enough?).
- Also, please try to fix that orrible loss calculation.
- Play with buffer bias.

Done:
buffer bias 2e-5 is 25kh, or 3 years. 5e-5 is 10kh or just over a year. 'k.
- After changing '6' to hours, test 2h global period.
- Different amount of weight decay might be needed for this larger amount of data as well.
- Run more iterations (120k, 160k, 200k)
- If weight decay is entirely unneeded, does that mean that we can get away with adding another (C)NN layer?
